{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aging-current",
   "metadata": {},
   "source": [
    "# Recycling Project Interview Assessment\n",
    "\n",
    "\n",
    "\n",
    "# Part 1b.\n",
    "Explain your HTTP verb choice in 1 to 2 sentences for each of your endpoints.\n",
    "    \n",
    "    - Endpoint 1: ('/') \n",
    "        - I used this home page to query all records on get request. \n",
    "        - I also used a post request here to delete individual record by ID; I put the delete request here just to prevent creating another function and minimizing code\n",
    "        \n",
    "    - Endpoint 2: ('/new-pet') \n",
    "        - I used this endpoint to post form data to create new pet entry as name would imply\n",
    "    \n",
    "    - Endpoint 3: ('/pet')\n",
    "        - I used get request here to query individual pet by ID\n",
    "        - I used post request here to update individual pet by ID \n",
    "    \n",
    "# Part 2a.\n",
    "Write a docker file for your flask app (including the sqlite database). The docker run command will be docker run -p 8881:8881\n",
    "\n",
    "    - Created working Dockerfile\n",
    "    - Created working docker-compose.yml\n",
    "\n",
    "    \n",
    "    \n",
    "# Part 2b.\n",
    "Explain how you would test your backend container locally. Please specify any tools or libraries that you think are relevant, as well as the specific tests that you would perform.\n",
    "\n",
    "        - You can create *.yml files to run with docker to test specified commands and outputs\n",
    "        - Create Bash Scripts to test container for: \n",
    "            - Networking (is sockets available outside container?)\n",
    "            - File System (file owners and permissions)\n",
    "        - Container Structure Tets (CST) - Open source tool developed by Google. Contains predefined set of tests\n",
    "        - Manual user tests\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# Part 3\n",
    "Imagine that sites contains a list of thousands of URLs. Re-write the following code to be more performant.\n",
    "    \n",
    "    - Solution 1: Multi-Threading with Thread Pool Executer\n",
    "        - Uses session objection connection pooling to reuse connection if targeting a site multiple times\n",
    "        - Uses ThreadPoolExecuter to create multi thread requests\n",
    "        \n",
    "    -Solution 2: Asynchronous \n",
    "        - I used asyncio package to create the optimal time efficient solution\n",
    "        - Asyncio uses a single threaded event loop which is even faster than multi-threading \n",
    "        - Aiohttp package is the asynchronous HTTP Client/Server for asyncio \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "meaningful-brighton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 14120 from http://www.google.com\n",
      "Read 80555 from http://www.facebook.com\n",
      "Read 625920 from http://www.youtube.com\n",
      "Read 292620 from http://www.github.com\n",
      "Read 14121 from http://www.google.com\n",
      "Read 719202 from http://www.yahoo.com\n",
      "Read 80551 from http://www.facebook.com\n",
      "Read 627284 from http://www.youtube.com\n",
      "Read 292620 from http://www.github.com\n",
      "Read 14123 from http://www.google.com\n",
      "Read 80552 from http://www.facebook.com\n",
      "Read 292620 from http://www.github.com\n",
      "Read 719185 from http://www.yahoo.com\n",
      "Read 719927 from http://www.yahoo.com\n",
      "Read 627961 from http://www.youtube.com\n",
      "download 15 links in 1.9816310405731201 seconds\n"
     ]
    }
   ],
   "source": [
    "# Solution 1: Multi Threading\n",
    "import requests\n",
    "from requests.sessions import Session\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Thread,local\n",
    "import time\n",
    "\n",
    "sites = [\n",
    "    \"http://www.google.com\",\n",
    "    \"http://www.github.com\",\n",
    "    \"http://www.youtube.com\",\n",
    "    \"http://www.facebook.com\",\n",
    "    \"http://www.yahoo.com\"\n",
    "]*3\n",
    "\n",
    "results = []\n",
    "thread_local = local()\n",
    "\n",
    "\n",
    "def get_session() -> Session:\n",
    "    if not hasattr(thread_local,'session'):\n",
    "        thread_local.session = requests.Session()\n",
    "    return thread_local.session\n",
    "\n",
    "def scrape(site:str):\n",
    "    session = get_session()\n",
    "    with session.get(site) as response:\n",
    "        print(f'Read {len(response.content)} from {site}')\n",
    "        results.append(response)\n",
    "\n",
    "\n",
    "def scrape_sites(sites):\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        executor.map(scrape,sites)\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "scrape_sites(sites)\n",
    "end = time.time()\n",
    "\n",
    "print(f'download {len(sites)} links in {end - start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-genetics",
   "metadata": {},
   "source": [
    "Solution 1 (multithreading):\n",
    "\n",
    "    completed in 1.98seconds which is about 2.5x faster than original code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "facial-membrane",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 14139 from http://www.google.com\n",
      "Read 14141 from http://www.google.com\n",
      "Read 14098 from http://www.google.com\n",
      "Read 292549 from http://www.github.com\n",
      "Read 292550 from http://www.github.com\n",
      "Read 80514 from http://www.facebook.com\n",
      "Read 629276 from http://www.youtube.com\n",
      "Read 292549 from http://www.github.com\n",
      "Read 711376 from http://www.yahoo.com\n",
      "Read 637227 from http://www.youtube.com\n",
      "Read 80509 from http://www.facebook.com\n",
      "Read 711304 from http://www.yahoo.com\n",
      "Read 620382 from http://www.youtube.com\n",
      "Read 80509 from http://www.facebook.com\n",
      "Read 711788 from http://www.yahoo.com\n",
      "download 15 links in 1.1307389736175537 seconds\n"
     ]
    }
   ],
   "source": [
    "# Solution 2: Asyncio\n",
    "import asyncio\n",
    "import time \n",
    "import aiohttp\n",
    "from aiohttp.client import ClientSession\n",
    "\n",
    "sites = [\n",
    "    \"http://www.google.com\",\n",
    "    \"http://www.github.com\",\n",
    "    \"http://www.youtube.com\",\n",
    "    \"http://www.facebook.com\",\n",
    "    \"http://www.yahoo.com\"\n",
    "]*3\n",
    "\n",
    "results = []\n",
    "\n",
    "async def scrape(site:str,session:ClientSession):\n",
    "    async with session.get(site) as response:\n",
    "        result = await response.text()\n",
    "        print(f'Read {len(result)} from {site}')\n",
    "        return result\n",
    "\n",
    "async def scrape_sites(sites):\n",
    "    connection = aiohttp.TCPConnector(limit=7)\n",
    "    async with aiohttp.ClientSession(connector=connection) as session:\n",
    "        for site in sites:\n",
    "            task = asyncio.ensure_future(scrape(site=site,session=session))\n",
    "            results.append(task)\n",
    "        await asyncio.gather(*results,return_exceptions=True) # the await must be nest inside of the session\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "asyncio.run(scrape_sites(sites))\n",
    "end = time.time()\n",
    "\n",
    "print(f'download {len(sites)} links in {end - start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-automation",
   "metadata": {},
   "source": [
    "Solution 2 (optimal):\n",
    "\n",
    "    The optimized asynchronous solution imporoved run-time from 5.18seconds to 1.13 seconds\n",
    "    This solution is about ~4.5x faster than the original base case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-friday",
   "metadata": {},
   "source": [
    "# Part 4\n",
    "Describe what happens in the steps between merging a pull request and new code running in production. Feel free to reference any relevant technologies or tools that you've used for things such as CICD pipelines, container registries, hosting, etc.\n",
    "\n",
    "- Hosting: Configure AWS CodePipeline to connect to github via webhooks for continuous integration and configure pipeline with AWS CodeDeploy to automatically update and deploy \n",
    "    - I've used Heroku to automatically deploy on git merge\n",
    "    - I've used Amplify & GCP to deploy from command line or github push\n",
    "    \n",
    "- CI/CD Pipeline:\n",
    "    - Jenkins: Build, Deploy, Test, Release\n",
    "        - Configure Jenkins to recieve new code merges via webhooks\n",
    "        - Configuer to build and deploy code and run unit tests and analysis and to generate health reports\n",
    "        - If no errors, Jenkins automates code to be dockerized by creating image and pushing to container registry\n",
    "        - Next step is further testing (database migration, container deployment, Security & Load testing)\n",
    "        - IFF, all steps above are passed, the code should be sent to Production Deployment\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
